{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Brandon's Test File.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OprhrKg7loxy",
        "-cFIJ-Ptl0gB",
        "FQAmrL1RlujQ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeUE1P2JRx2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install livelossplot==0.5.1 --quiet\n",
        "\n",
        "import gc\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.optim as optim\n",
        "import cv2 as cv\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "from skimage.transform import resize \n",
        "from torch.utils.data import random_split, DataLoader, Dataset, ConcatDataset\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "import os\n",
        "from os import listdir\n",
        "from sys import getsizeof"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0pvkD7rJ5BL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class h5DatasetSlice(Dataset):\n",
        "\n",
        "    def __init__(self, file_path, func=None, portion=(0, 1)):\n",
        "        '''\n",
        "        h5DatasetSlice creates a custom dataset object and allows for slicing\n",
        "        of the source data to produce training, validation, and testing sets.\n",
        "        An optional parameter 'func' can be passed in for data augmentation.\n",
        "        '''\n",
        "        \n",
        "        # file_path: path to .h5 file\n",
        "        self.file = h5py.File(file_path + \"data.h5\", \"r\")\n",
        "        self.func = func\n",
        "\n",
        "        self.rgb_dbs = sorted([key for key in self.file.keys() \\\n",
        "                                if key[:3] == \"rgb\"])\n",
        "        self.depth_dbs = sorted([key for key in self.file.keys() \\\n",
        "                                if key[:5] == \"depth\"])\n",
        "        portion = [int(k*len(self.rgb_dbs)) for k in portion]\n",
        "\n",
        "        self.rgb_dbs = self.rgb_dbs[portion[0] : portion[1]]\n",
        "        self.depth_dbs = self.depth_dbs[portion[0] : portion[1]]\n",
        "        \n",
        "        self.totens = transforms.ToTensor()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        rgb = self.file[self.rgb_dbs[index]][:]\n",
        "        depth = self.file[self.depth_dbs[index]][:]\n",
        "\n",
        "        if self.func:\n",
        "            rgb, depth = self.func(rgb, depth)\n",
        "        else:\n",
        "            rgb = self.totens(rgb)\n",
        "            depth = self.totens(depth.astype(np.float64) / 65535.0)\n",
        "            \n",
        "        return rgb, depth\n",
        "\n",
        "    \n",
        "    def __len__(self):\n",
        "        return min(len(self.rgb_dbs), len(self.depth_dbs))\n",
        "\n",
        "def alter(rgb, depth):\n",
        "\n",
        "    totens = transforms.ToTensor()\n",
        "\n",
        "    rgb, depth = random_rotate((rgb, depth), max_angle=15, fill=1)\n",
        "    rgb, depth = random_hflip((rgb, depth))\n",
        "    rgb = TF.center_crop(rgb, (400, 533))\n",
        "    depth = TF.center_crop(depth, (400, 533))\n",
        "    rgb = resize(np.array(rgb), (225, 300))\n",
        "    depth = resize(np.array(depth), (225, 300))\n",
        "    rgb, depth = random_crop((np.array(rgb, dtype='float64'), np.array(depth, dtype='float64')), (224, 224))\n",
        "\n",
        "    rgb = totens(rgb)\n",
        "    depth = totens(depth)\n",
        "\n",
        "    depth = transforms.Normalize(0,0.1217)(depth)\n",
        "\n",
        "    return rgb, depth\n",
        "\n",
        "def alter_val(rgb, depth):\n",
        "    '''\n",
        "    alter_val augments the dataset by cropping but avoids rotations or flips\n",
        "    '''\n",
        "\n",
        "    totens = transforms.ToTensor()\n",
        "\n",
        "    rgb = TF.center_crop(Image.fromarray(rgb), (400, 533))\n",
        "    depth = TF.center_crop(Image.fromarray(depth), (400, 533))\n",
        "    rgb = resize(np.array(rgb), (224, 224))\n",
        "    depth = resize(np.array(depth), (224, 224))\n",
        "\n",
        "    rgb = totens(rgb)\n",
        "    depth = totens(depth)\n",
        "\n",
        "    depth = transforms.Normalize(0,0.1217)(depth)\n",
        "\n",
        "    return rgb, depth\n",
        "\n",
        "def load_dataset(file_path, batch_size, ratios=(.7, .15, .15), shuffle=True):\n",
        "    train_data = h5DatasetSlice(file_path=file_path, func=alter, \n",
        "                                portion=(0, ratios[0]))\n",
        "    val_data = h5DatasetSlice(file_path=file_path, func=alter_val,\n",
        "                                portion=(ratios[0], ratios[0]+ratios[1]))\n",
        "    test_data = h5DatasetSlice(file_path=file_path, func=alter_val,\n",
        "                                portion=(ratios[0]+ratios[1], 1))\n",
        "    \n",
        "    print(len(train_data), len(val_data), len(test_data))\n",
        "    \n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, \n",
        "                              shuffle=shuffle, num_workers=0, drop_last=True)\n",
        "    val_loader = DataLoader(val_data, batch_size=batch_size, \n",
        "                              shuffle=shuffle, num_workers=0, drop_last=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, \n",
        "                              shuffle=shuffle, num_workers=0, drop_last=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULbUcxoGMDtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = \"/content/drive/My Drive/APS360_Project/Data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJPIEN5ISFAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipUpconv2(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activ=nn.LeakyReLU(0.2)):\n",
        "        super(SkipUpconv2, self).__init__()\n",
        "\n",
        "        mid_channels = (in_channels + out_channels) // 2\n",
        "        self.activ = activ\n",
        "        #self.norm1 = nn.BatchNorm2d(mid_channels)\n",
        "        #self.norm2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels = mid_channels,\n",
        "            kernel_size = 3,\n",
        "            stride = 1,\n",
        "            padding = 1,\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=mid_channels,\n",
        "            out_channels = out_channels,\n",
        "            kernel_size = 3,\n",
        "            stride = 1,\n",
        "            padding = 1,\n",
        "        )\n",
        "\n",
        "    def forward(self, small, big):\n",
        "        scaled = F.interpolate(small, size=big.shape[2:], mode='bilinear', \n",
        "                               align_corners=True)\n",
        "        x = self.activ(self.conv1(torch.cat((scaled, big), dim=1)))\n",
        "        x = self.activ(self.conv2(x))\n",
        "\n",
        "        return F.interpolate(x, scale_factor=2, mode='bilinear', \n",
        "                             align_corners=True)\n",
        "\n",
        "def vecinfo(vec):\n",
        "    totens = transforms.ToTensor()\n",
        "    try:\n",
        "        vec = totens(vec)\n",
        "    except:\n",
        "        pass\n",
        "    print(\"min: {}, max: {}, mean: {}\".format(\n",
        "        torch.min(vec),\n",
        "        torch.max(vec),\n",
        "        torch.mean(vec)\n",
        "    ))\n",
        "\n",
        "\n",
        "def random_crop(data, size):\n",
        "    rgb, depth = data\n",
        "    h, w = rgb.shape[:2]\n",
        "    new_h, new_w = size\n",
        "\n",
        "    top = np.random.randint(0, h - new_h)\n",
        "    left = np.random.randint(0, w - new_w)\n",
        "\n",
        "    rgb = rgb[top: top + new_h,\n",
        "              left: left + new_w]\n",
        "    depth = depth[top: top + new_h,\n",
        "                  left: left + new_w]\n",
        "    \n",
        "    return rgb, depth\n",
        "\n",
        "def random_rotate(data, max_angle, resample=False, expand=False, \n",
        "                  center=None, fill=None):\n",
        "    rgb, depth = data\n",
        "    rgb = Image.fromarray(rgb)\n",
        "    depth = Image.fromarray(depth)\n",
        "    angle = np.random.uniform(-max_angle, max_angle)\n",
        "    return (TF.rotate(rgb, angle, resample, expand, center, fill),\n",
        "            TF.rotate(depth, angle, resample, expand, center, fill))\n",
        "\n",
        "def random_hflip(data, p=0.5):\n",
        "    rgb, depth = data\n",
        "\n",
        "    if torch.rand(1) < p:\n",
        "        return TF.hflip(rgb), TF.hflip(depth)\n",
        "    \n",
        "    return rgb, depth\n",
        "\n",
        "def valid_loss(model, criterion, val_loader, index):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, (rgb, depth) in enumerate(val_loader, \\\n",
        "                                         index % (len(val_loader) - 1)):\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                rgb = rgb.cuda()\n",
        "                depth = depth.cuda()\n",
        "            \n",
        "            pred = model(rgb.float())\n",
        "            del rgb\n",
        "\n",
        "            return torch.sqrt(criterion(depth.squeeze(), pred.squeeze())).item()\n",
        "\n",
        "\n",
        "def train(model, batch_size=16, lr=0.01, num_epochs=1, ratios=(.8, .1, .1),\n",
        "          load_pretrained=False):\n",
        "\n",
        "    plotlosses = PlotLosses(groups={'Loss': ['train', 'val']})\n",
        "    \n",
        "    torch.manual_seed(1)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    checkpoint_filepath = \"/content/drive/My Drive/APS360_Project/Checkpoints/demo_model.pth\"\n",
        "\n",
        "    if not os.path.exists(os.path.dirname(checkpoint_filepath)):\n",
        "      print(\"Checkpoint folder not found. Creating new folder\")\n",
        "      os.makedirs(os.path.dirname(checkpoint_filepath))\n",
        "    else:\n",
        "      print(\"Checkpoint folder found.\")\n",
        "\n",
        "    if load_pretrained and os.path.exists(checkpoint_filepath):\n",
        "      checkpoint = torch.load(checkpoint_filepath)\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      print(\"Resuming training from epoch\",checkpoint['epoch'],\"\\nLoss:\", checkpoint['loss'])\n",
        "    else:\n",
        "      print(\"No previous checkpoint file found.\")\n",
        "\n",
        "    print(\"Loading data...\")\n",
        "    train_loader, val_loader = load_dataset(\n",
        "      file_path=filepath,\n",
        "      batch_size=batch_size,\n",
        "      ratios=ratios\n",
        "      )[:2]\n",
        "    print(\"Data loading complete\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        for i, (rgb, depth) in enumerate(train_loader):\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                rgb = rgb.cuda()\n",
        "                depth = depth.cuda()\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.set_grad_enabled(True):\n",
        "\n",
        "                pred = model(rgb.float())\n",
        "                del rgb\n",
        "\n",
        "                loss = torch.sqrt(criterion(depth.squeeze(), pred.squeeze()))\n",
        "\n",
        "                gc.collect()\n",
        "            \n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            val_loss = valid_loss(model, nn.MSELoss(), val_loader, i)\n",
        "\n",
        "            plotlosses.update({\n",
        "                'train': loss.item(),\n",
        "                'val': val_loss\n",
        "            })\n",
        "            plotlosses.send()\n",
        "\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'loss': loss.item(),\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict()\n",
        "            }, checkpoint_filepath)\n",
        "\n",
        "            plt.imshow(depth.cpu().detach().numpy()[0].squeeze(), cmap='viridis')\n",
        "            plt.show()\n",
        "            plt.imshow(pred.cpu().detach().numpy()[0].squeeze(), cmap='viridis')\n",
        "            plt.show()\n",
        "            del depth, pred\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT1MCbbST2rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipEncoder6(nn.Module):\n",
        "    def __init__(self, channels=(640, 300, 150, 50)):\n",
        "        super(SkipEncoder6, self).__init__()\n",
        "\n",
        "        self.mobilenet = models.mobilenet_v2(pretrained=True).cuda()\n",
        "\n",
        "        self.norm1 = nn.BatchNorm2d(channels[1])\n",
        "        self.norm2 = nn.BatchNorm2d(channels[3])\n",
        "\n",
        "        self.dec1 = nn.Conv2d(1280, channels[0], kernel_size=3, stride=1, \n",
        "                              padding=1)\n",
        "        '''\n",
        "        the numbers added to the input channels are the number of channels in \n",
        "        the mobilenet layers that will be \n",
        "        '''\n",
        "        self.dec2 = SkipUpconv2(channels[0]+96, channels[1])\n",
        "        self.dec3 = SkipUpconv2(channels[1]+32, channels[2])\n",
        "        self.dec4 = SkipUpconv2(channels[2]+24, channels[3])\n",
        "        self.dec5 = SkipUpconv2(channels[3]+16, 1)\n",
        "\n",
        "        self.sig = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x0):\n",
        "        '''\n",
        "        passes the rgb image into mobilenet and appends the output of each layer\n",
        "        into the features list. This saves the progress of the feature maps\n",
        "        as they progress through the network. Any of those outputs can be fed \n",
        "        into dec1, dec2, etc. which are objects of the SkipUpconv class. \n",
        "        Basically this allows us to have skip connections from inside the \n",
        "        pretrained model encoder going into the custom layers in the decoder.\n",
        "        Use this code to better understand the features list and how to properly \n",
        "        index it to extract the outputs you want:\n",
        "\n",
        "        for feature in features:\n",
        "            print(feature.shape)\n",
        "        \n",
        "        note: put this after the for loop below\n",
        "        '''\n",
        "        features = [x0]\n",
        "        for name, module in self.mobilenet.features._modules.items():\n",
        "            features.append(module(features[-1]))\n",
        "        \n",
        "        '''\n",
        "        introduce skip connections from the encoder into the decoder portion of\n",
        "        the model. This allows the trainable decoder to have access to less \n",
        "        processed and less abstract versions of the input image, which makes it \n",
        "        better at capturing geometric details. There are also two normalizations \n",
        "        spread out between the upconvolutions.\n",
        "        '''\n",
        "        x1 = self.dec1(features[19])\n",
        "        x2 = self.norm1(self.dec2(x1, features[14]))\n",
        "        x3 = self.dec3(x2, features[7])\n",
        "        x4 = self.norm2(self.dec4(x3, features[4]))\n",
        "        x5 = self.dec5(x4, features[2])\n",
        "\n",
        "        return self.sig(x5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geRTLYSi1PN3",
        "colab_type": "text"
      },
      "source": [
        "# Skip Encoder 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyPqfInq1SQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "skipenc6 = SkipEncoder6()\n",
        "\n",
        "print(sum(p.numel() for p in skipenc6.parameters()))\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU available\")\n",
        "    skipenc6.cuda()\n",
        "\n",
        "#TRAIN FOR AROUND 300 BATCHES OR SO\n",
        "\n",
        "train(skipenc6, batch_size=32, lr=1e-4, num_epochs=5, ratios=(.8, .15, .05), load_pretrained=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}